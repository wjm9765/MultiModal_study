{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMis/b8mhdp2U5IIps2p6Yk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHLE-l9BQ5AK"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch Pillow bitsandbytes accelerate\n",
        "#êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/COCO_Caption_10k\"\n",
        "IMAGE_DIR = os.path.join(DATA_PATH, \"images\")\n",
        "METADATA_FILE = os.path.join(DATA_PATH, \"metadata.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SPvbDX0tmgNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "if not os.path.exists(METADATA_FILE):\n",
        "    print(\"ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥í•©ë‹ˆë‹¤. (ìº¡ì…˜ 5ê°œ ëª¨ë‘ í™œìš©)\")\n",
        "\n",
        "    dataset = load_dataset(\"lmms-lab/COCO-Caption2017\", split='val', streaming=True)\n",
        "    subset = dataset.take(10000)\n",
        "\n",
        "    total_captions = 0\n",
        "    with open(METADATA_FILE, 'w') as f:\n",
        "        for i, example in enumerate(subset):\n",
        "            if i % 500 == 0: print(f\"{i}ë²ˆì§¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘... (ì´ ìº¡ì…˜ ìˆ˜: {total_captions})\")\n",
        "            image = example['image'].convert(\"RGB\")\n",
        "\n",
        "            # ì´ë¯¸ì§€ëŠ” í•œ ë²ˆë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "            image_filename = example['file_name']\n",
        "            image_path = os.path.join(IMAGE_DIR, image_filename)\n",
        "            if not os.path.exists(image_path):\n",
        "                image.save(image_path)\n",
        "\n",
        "            for caption in example['answer']:\n",
        "                # ë™ì¼í•œ ì´ë¯¸ì§€ ê²½ë¡œì— ëŒ€í•´ ë‹¤ë¥¸ ìº¡ì…˜ì„ ê°€ì§„ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "                metadata_item = {\"image_path\": image_path, \"caption\": caption}\n",
        "                f.write(json.dumps(metadata_item) + \"\\n\")\n",
        "                total_captions += 1\n",
        "            # ------------------------------------\n",
        "\n",
        "    print(f\"ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! 10000ê°œ ì´ë¯¸ì§€ë¡œë¶€í„° ì´ {total_captions}ê°œì˜ ìº¡ì…˜ ë°ì´í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    print(\"ë°ì´í„°ì…‹ì´ ì´ë¯¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n"
      ],
      "metadata": {
        "id": "90W6BdwVj_6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    CLIPVisionModel,\n",
        "    CLIPImageProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "yJvvRGFnnIBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    llm_model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "    clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # ë°ì´í„° ê²½ë¡œ\n",
        "\n",
        "    DATA_PATH = \"/content/drive/MyDrive/COCO_Caption_10k\"\n",
        "    IMAGE_DIR = os.path.join(DATA_PATH, \"images\")\n",
        "    METADATA_FILE = os.path.join(DATA_PATH, \"metadata.jsonl\")\n",
        "\n",
        "    # í›ˆë ¨ ë°˜ë³µ íšŸìˆ˜ (ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ë³€ìˆ˜ë¡œ ì§€ì •)\n",
        "    train_epochs = 7\n",
        "\n",
        "    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë° í•™ìŠµë¥ \n",
        "    batch_size = 4\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    # ì €ì¥ ê²½ë¡œ\n",
        "    # 1. ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•  í´ë”\n",
        "    initial_projector_dir = \"/content/drive/MyDrive/LLava_stage0_first\"\n",
        "    # 2. í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë° ìµœì¢… ê²°ê³¼ë¬¼ì„ ì €ì¥í•  í´ë”\n",
        "    output_dir = \"/content/drive/MyDrive/llava_stage1_checkpoint\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
        "os.makedirs(config.initial_projector_dir, exist_ok=True)\n",
        "os.makedirs(config.output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "rP2OXUPpnvDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMì„ 4-bitë¡œ ë¡œë”©í•˜ì—¬ VRAM ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì…ë‹ˆë‹¤.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# LLM (Vicuna) ë° í† í¬ë‚˜ì´zer ë¡œë”©\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.llm_model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    #device_map=\"auto\" # GPUì— ìë™ìœ¼ë¡œ ëª¨ë¸ ë ˆì´ì–´ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "mG1HQd5VoW2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.llm_model_name, use_fast=False)\n",
        "# ğŸ§  í•µì‹¬ ë‹¨ê³„: ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìœ„í•œ íŠ¹ìˆ˜ í† í° '<image>' ì¶”ê°€\n",
        "IMAGE_TOKEN = \"<image>\"\n",
        "if IMAGE_TOKEN not in tokenizer.get_added_vocab():\n",
        "    # í† í¬ë‚˜ì´ì €ì— <image> í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "    tokenizer.add_tokens([IMAGE_TOKEN], special_tokens=True)\n",
        "    # ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸°ë¥¼ ìƒˆ í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.\n",
        "    llm_model.resize_token_embeddings(len(tokenizer))\n",
        "    print(f\"'{IMAGE_TOKEN}' í† í°ì„ ì¶”ê°€í•˜ê³  ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆë¥¼ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# VicunaëŠ” ê¸°ë³¸ padding tokenì´ ì—†ìœ¼ë¯€ë¡œ, unknown tokenìœ¼ë¡œ ì„¤ì •í•´ì¤ë‹ˆë‹¤.\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "IMAGE_TOKEN_ID = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n",
        "\n",
        "# ì´ë¯¸ì§€ ì¸ì½”ë” (CLIP) ë° ì „ì²˜ë¦¬ê¸° ë¡œë”©\n",
        "vision_encoder = CLIPVisionModel.from_pretrained(config.clip_model_name).to(llm_model.device)\n",
        "image_processor = CLIPImageProcessor.from_pretrained(config.clip_model_name)\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "wsqkf6tNyarR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaVADataset(Dataset):\n",
        "    # def __init__(self, metadata_file, tokenizer, image_processor):\n",
        "    #     self.tokenizer = tokenizer\n",
        "    #     self.image_processor = image_processor\n",
        "    #     self.data = []\n",
        "    #     with open(metadata_file, 'r') as f:\n",
        "    #         for line in f:\n",
        "    #             self.data.append(json.loads(line))\n",
        "    def __init__(self, metadata_file, tokenizer, image_processor):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_processor = image_processor\n",
        "        self.data = []\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            # íŒŒì¼ì˜ ê° ì¤„ì„ ìˆœíšŒí•˜ë©° ë°ì´í„°ë¥¼ self.dataì— ì¶”ê°€\n",
        "            for i, line in enumerate(f):\n",
        "                # â­ï¸ í•µì‹¬: 1000ê°œì˜ ë°ì´í„°ë§Œ ë¡œë“œí•˜ê³  ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n",
        "                if i >= 2000:\n",
        "                    break\n",
        "                self.data.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = item['image_path']\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            pixel_values = self.image_processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        except Exception:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        caption = item['caption']\n",
        "        text_prompt = f\"USER: {IMAGE_TOKEN}\\nDescribe this image.\\nASSISTANT: {caption}{self.tokenizer.eos_token}\"\n",
        "        tokenized_output = self.tokenizer(text_prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "        input_ids = tokenized_output['input_ids'].squeeze(0)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        prompt_part = text_prompt.split(\"ASSISTANT:\")[0] + \"ASSISTANT:\"\n",
        "        prompt_tokens = self.tokenizer(prompt_part, return_tensors=\"pt\")['input_ids']\n",
        "        mask_until_index = prompt_tokens.shape[1]\n",
        "        labels[:mask_until_index] = -100\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values.to(llm_model.dtype),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "# =================================================================================\n",
        "# 5. LLaVA ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ë° ì´ˆê¸° ê°€ì¤‘ì¹˜ ì €ì¥\n",
        "# =================================================================================\n",
        "class LLaVAModel(nn.Module):\n",
        "    def __init__(self, vision_encoder, llm_model, image_token_id):\n",
        "        super().__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.llm_model = llm_model\n",
        "        self.image_token_id = image_token_id\n",
        "\n",
        "        # 1. í”„ë¡œì ì…˜ ë ˆì´ì–´ ì •ì˜: CLIP ì¶œë ¥ ì°¨ì› -> LLM ì…ë ¥ ì°¨ì›\n",
        "        # n*m : m ì°¨ì›ì—ì„œ n ì°¨ì›ìœ¼ë¡œ\n",
        "        self.projection_layer = nn.Linear(\n",
        "            self.vision_encoder.config.hidden_size,\n",
        "            self.llm_model.config.hidden_size\n",
        "        )\n",
        "\n",
        "        for param in self.vision_encoder.parameters(): param.requires_grad = False\n",
        "        for param in self.llm_model.parameters(): param.requires_grad = False\n",
        "        for param in self.projection_layer.parameters(): param.requires_grad = True\n",
        "\n",
        "    def forward(self, pixel_values, input_ids, labels=None, **kwargs):\n",
        "        image_outputs = self.vision_encoder(pixel_values=pixel_values, return_dict=True)\n",
        "        image_features = image_outputs.pooler_output\n",
        "        projected_image_features = self.projection_layer(image_features)\n",
        "        word_embeddings = self.llm_model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        for i in range(input_ids.shape[0]):\n",
        "            image_token_index = (input_ids[i] == self.image_token_id).nonzero(as_tuple=True)[0]\n",
        "            if image_token_index.numel() > 0:\n",
        "                word_embeddings[i, image_token_index] = projected_image_features[i].to(word_embeddings.dtype)\n",
        "\n",
        "        return self.llm_model(inputs_embeds=word_embeddings, labels=labels, return_dict=True)\n",
        "\n",
        "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "model = LLaVAModel(vision_encoder, llm_model, IMAGE_TOKEN_ID)\n",
        "final_projector_path = os.path.join(config.output_dir, \"final_projection_layer.pt\")\n",
        "\n",
        "# ë§Œì•½ ì´ì „ì— í•™ìŠµí•˜ê³  ì €ì¥í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´, í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "if os.path.exists(final_projector_path):\n",
        "    print(f\"âœ… ì´ì „ì— í•™ìŠµëœ í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ {final_projector_path}ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n",
        "    # torch.loadë¡œ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, model.projection_layer.load_state_dictë¡œ ì ìš©í•©ë‹ˆë‹¤.\n",
        "    model.projection_layer.load_state_dict(torch.load(final_projector_path))\n",
        "else:\n",
        "    # í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìœ¼ë©´ (ê°€ì¥ ì²˜ìŒ ì‹¤í–‰í•˜ëŠ” ê²½ìš°), ìƒˆë¡œìš´ ë ˆì´ì–´ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "    print(\"ğŸ”µ í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ, ìƒˆë¡œìš´ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "    # ì´ ê²½ìš°ì—ë§Œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    initial_projector_path = os.path.join(config.initial_projector_dir, \"initial_projection_layer.pt\")\n",
        "    torch.save(model.projection_layer.state_dict(), initial_projector_path)\n",
        "    print(f\"âœ¨ ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ {initial_projector_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "#\n",
        "##if not os.path.exists(config.initial_projector_dir):\n",
        "  #í•™ìŠµ ì „, ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
        "  #initial_projector_path = os.path.join(config.initial_projector_dir, \"initial_projection_layer.pt\")\n",
        "  #torch.save(model.projection_layer.state_dict(), initial_projector_path)\n",
        "  #print(f\"âœ… ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ {initial_projector_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n"
      ],
      "metadata": {
        "id": "wg2lCr7ppYX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "train_dataset = LLaVADataset(config.METADATA_FILE, tokenizer, image_processor)\n",
        "\n",
        "# í•™ìŠµ ì¸ì ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config.output_dir,\n",
        "    num_train_epochs=config.train_epochs,\n",
        "    per_device_train_batch_size=config.batch_size,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=config.learning_rate,\n",
        "    logging_steps=20,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer ì •ì˜\n",
        "trainer = Trainer( #í—ˆê¹…í˜ì´ìŠ¤ í•¨ìˆ˜ ì‚¬ìš©\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘!\n",
        "trainer.train()\n",
        "print(f\"{config.train_epochs} ë²ˆ ìµœì¢… í•™ìŠµ ì¢…ë£Œ\")\n",
        "\n",
        "# í•™ìŠµ ì™„ë£Œ í›„, ìµœì¢… í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥\n",
        "final_projector_path = os.path.join(config.output_dir, \"final_projection_layer.pt\")\n",
        "torch.save(model.projection_layer.state_dict(), final_projector_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "k5YC0m5RqaXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹¤ìŠµ ê²°ê³¼\n",
        "### 1. Projection Layer í¬ê¸°\n",
        "\n",
        "### 1000ê°œë¡œ 20ë²ˆ 2000ê°œë¡œ 20ë²ˆ"
      ],
      "metadata": {
        "id": "cHZKT47Qot25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. í”„ë¡œì ì…˜ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜(weight) í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "projection_layer_shape = model.projection_layer.weight.shape\n",
        "\n",
        "#    ì…ë ¥ ì°¨ì›: CLIP Vision Encoderì˜ ì¶œë ¥ ì°¨ì›\n",
        "#    ì¶œë ¥ ì°¨ì›: LLM (Vicuna)ì˜ ì…ë ¥ ì„ë² ë”© ì°¨ì›\n",
        "input_dim = model.vision_encoder.config.hidden_size\n",
        "output_dim = model.llm_model.config.hidden_size\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ğŸ” Projection Layer í¬ê¸° ì •ë³´\")\n",
        "print(f\"  - ë ˆì´ì–´ì˜ ì „ì²´ ê°€ì¤‘ì¹˜ í–‰ë ¬ í¬ê¸° (shape): {projection_layer_shape}\")\n",
        "print(f\"  - ì…ë ¥ ì°¨ì› (Input Dimension): {input_dim} (CLIP Vision Encoderì˜ ì¶œë ¥)\")\n",
        "print(f\"  - ì¶œë ¥ ì°¨ì› (Output Dimension): {output_dim} (Vicuna LLMì˜ ì…ë ¥)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ================================================================================="
      ],
      "metadata": {
        "id": "HUM71POdoyCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ì„ì˜ì˜ ë°ì´í„° ì…‹ì— ëŒ€í•´ ì •ë‹µ ìº¡ì…˜ê³¼ ì„ë² ë”© ì°¨ì´ ë¹„êµ\n",
        "\n",
        "### í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•ŠìŒ 500ê°œ ì´ë¯¸ì§€-ìº¡ì…˜ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
        "\n"
      ],
      "metadata": {
        "id": "z9A3pKh_oztk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "all_data = []\n",
        "with open(config.METADATA_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        all_data.append(json.loads(line))\n",
        "\n",
        "eval_data = all_data[-500:]\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ§ª í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: ì´ë¯¸ì§€-ìº¡ì…˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸¡ì •\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# í‰ê°€ë¥¼ ìœ„í•´ ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •\n",
        "model.eval()\n",
        "\n",
        "# LLMì˜ ë‹¨ì–´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "word_embedding_layer = model.llm_model.get_input_embeddings()\n",
        "\n",
        "device = model.llm_model.device\n",
        "total_similarity = 0.0\n",
        "evaluated_count = 0\n",
        "\n",
        "try:\n",
        "    # final_projector_path ë³€ìˆ˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    print(f\"ğŸ”µ '{final_projector_path}'ì—ì„œ ìµœì¢… í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n",
        "    # ì§€ì •ëœ ê²½ë¡œì—ì„œ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ í˜„ì¬ ëª¨ë¸ì˜ projection_layerì— ì ìš©í•©ë‹ˆë‹¤.\n",
        "    model.projection_layer.load_state_dict(torch.load(final_projector_path))\n",
        "    print(\"âœ… ìµœì¢… ê°€ì¤‘ì¹˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì— ì ìš©í–ˆìŠµë‹ˆë‹¤.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ì—ëŸ¬: ìµœì¢… ê°€ì¤‘ì¹˜ íŒŒì¼ '{final_projector_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í‰ê°€ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n",
        "    exit()\n",
        "\n",
        "\n",
        "# í‰ê°€ ì¤‘ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ torch.no_grad() ì‚¬ìš©\n",
        "with torch.no_grad():\n",
        "    # í‰ê°€ ë°ì´í„°(ë§ˆì§€ë§‰ 500ê°œ)ë¥¼ í•˜ë‚˜ì”© ìˆœíšŒ\n",
        "    for item in tqdm(eval_data, desc=\"Evaluating Cosine Similarity\"):\n",
        "        try:\n",
        "            # 1. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±\n",
        "            image_path = item['image_path']\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "            # ì´ë¯¸ì§€ë¥¼ CLIP í”„ë¡œì„¸ì„œë¡œ ì „ì²˜ë¦¬í•˜ê³  ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            pixel_values = image_processor(image, return_tensors=\"pt\")['pixel_values'].to(device, dtype=torch.float16)\n",
        "\n",
        "            # Vision Encoderë¥¼ í†µê³¼ì‹œì¼œ ì´ë¯¸ì§€ í”¼ì²˜ ì¶”ì¶œ\n",
        "            image_outputs = model.vision_encoder(pixel_values=pixel_values, return_dict=True)\n",
        "            image_features = image_outputs.pooler_output\n",
        "\n",
        "            # í•™ìŠµëœ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±\n",
        "            image_embedding = model.projection_layer(image_features) # shape: (1, 4096)\n",
        "\n",
        "            # 2. ìº¡ì…˜ ì²˜ë¦¬ ë° ìº¡ì…˜ ì„ë² ë”© ìƒì„±\n",
        "            caption = item['caption']\n",
        "\n",
        "            # ìº¡ì…˜ì„ í† í¬ë‚˜ì´ì§•í•˜ê³  ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            input_ids = tokenizer(caption, return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "            # í† í° IDë¥¼ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\n",
        "            caption_token_embeddings = word_embedding_layer(input_ids) # shape: (1, sequence_length, 4096)\n",
        "\n",
        "            # ë¬¸ì¥ ì „ì²´ë¥¼ ëŒ€í‘œí•˜ëŠ” í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ì–»ê¸° ìœ„í•´ í‰ê· ì„ ê³„ì‚° (íŒ¨ë”© í† í°ì€ ì œì™¸)\n",
        "            caption_embedding = caption_token_embeddings.mean(dim=1) # shape: (1, 4096)\n",
        "\n",
        "            # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            similarity = F.cosine_similarity(image_embedding, caption_embedding)\n",
        "            total_similarity += similarity.item()\n",
        "            evaluated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # íŒŒì¼ì´ ê¹¨ì ¸ìˆê±°ë‚˜ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€\n",
        "            print(f\"Skipping {item.get('image_path', 'unknown image')} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "# ìµœì¢… í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ì¶œë ¥\n",
        "if evaluated_count > 0:\n",
        "    average_similarity = total_similarity / evaluated_count\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ¨ í‰ê°€ ê²°ê³¼ âœ¨\")\n",
        "    print(f\"  - í‰ê°€ëœ ë°ì´í„° ê°œìˆ˜: {evaluated_count} / {len(eval_data)}\")\n",
        "    print(f\"  - í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {average_similarity:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "D7qgJ-lZo6Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. ì´ˆê¸° í–‰ë ¬ ê°’ê³¼ í›ˆë ¨ í›„ í–‰ë ¬ ê°’ ë™ì¼í•œ ì„ë² ë”© ë³€í™˜ ì°¨ì´ ë¹„êµ"
      ],
      "metadata": {
        "id": "IdpRgP1uo57e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "all_data = []\n",
        "# config ë³€ìˆ˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "with open(config.METADATA_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        all_data.append(json.loads(line))\n",
        "\n",
        "eval_data = all_data[-500:]\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ§ª ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ë¡œ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- [ìš”ì²­í•˜ì‹  ìˆ˜ì • ì‚¬í•­] ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° ---\n",
        "# ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì— ì •ì˜ëœ config ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸° ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "initial_projector_path = os.path.join(config.initial_projector_dir, \"initial_projection_layer.pt\")\n",
        "\n",
        "try:\n",
        "    print(f\"ğŸ”µ '{initial_projector_path}'ì—ì„œ ì´ˆê¸° í”„ë¡œì ì…˜ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n",
        "    # ì§€ì •ëœ ê²½ë¡œì—ì„œ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ í˜„ì¬ ëª¨ë¸ì˜ projection_layerì— ì ìš©í•©ë‹ˆë‹¤.\n",
        "    model.projection_layer.load_state_dict(torch.load(initial_projector_path))\n",
        "    print(\"âœ… ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ ëª¨ë¸ì— ì ìš©í–ˆìŠµë‹ˆë‹¤.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ì—ëŸ¬: ì´ˆê¸° ê°€ì¤‘ì¹˜ íŒŒì¼ '{initial_projector_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í‰ê°€ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n",
        "    exit()\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# í‰ê°€ë¥¼ ìœ„í•´ ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •\n",
        "model.eval()\n",
        "\n",
        "# LLMì˜ ë‹¨ì–´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "word_embedding_layer = model.llm_model.get_input_embeddings()\n",
        "\n",
        "device = model.llm_model.device\n",
        "total_similarity = 0.0\n",
        "evaluated_count = 0\n",
        "\n",
        "# í‰ê°€ ì¤‘ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ torch.no_grad() ì‚¬ìš©\n",
        "with torch.no_grad():\n",
        "    # í‰ê°€ ë°ì´í„°(ë§ˆì§€ë§‰ 500ê°œ)ë¥¼ í•˜ë‚˜ì”© ìˆœíšŒ\n",
        "    for item in tqdm(eval_data, desc=\"Evaluating with Initial Weights\"):\n",
        "        try:\n",
        "            # 1. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±\n",
        "            image_path = item['image_path']\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "            # ì´ë¯¸ì§€ë¥¼ CLIP í”„ë¡œì„¸ì„œë¡œ ì „ì²˜ë¦¬í•˜ê³  ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            pixel_values = image_processor(image, return_tensors=\"pt\")['pixel_values'].to(device, dtype=torch.float16)\n",
        "\n",
        "            # Vision Encoderë¥¼ í†µê³¼ì‹œì¼œ ì´ë¯¸ì§€ í”¼ì²˜ ì¶”ì¶œ\n",
        "            image_outputs = model.vision_encoder(pixel_values=pixel_values, return_dict=True)\n",
        "            image_features = image_outputs.pooler_output\n",
        "\n",
        "            # í•™ìŠµëœ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±\n",
        "            image_embedding = model.projection_layer(image_features) # shape: (1, 4096)\n",
        "\n",
        "            # 2. ìº¡ì…˜ ì²˜ë¦¬ ë° ìº¡ì…˜ ì„ë² ë”© ìƒì„±\n",
        "            caption = item['caption']\n",
        "\n",
        "            # ìº¡ì…˜ì„ í† í¬ë‚˜ì´ì§•í•˜ê³  ëª¨ë¸ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            input_ids = tokenizer(caption, return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "            # í† í° IDë¥¼ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\n",
        "            caption_token_embeddings = word_embedding_layer(input_ids) # shape: (1, sequence_length, 4096)\n",
        "\n",
        "            # ë¬¸ì¥ ì „ì²´ë¥¼ ëŒ€í‘œí•˜ëŠ” í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ì–»ê¸° ìœ„í•´ í‰ê· ì„ ê³„ì‚° (íŒ¨ë”© í† í°ì€ ì œì™¸)\n",
        "            caption_embedding = caption_token_embeddings.mean(dim=1) # shape: (1, 4096)\n",
        "\n",
        "            # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            similarity = F.cosine_similarity(image_embedding, caption_embedding)\n",
        "            total_similarity += similarity.item()\n",
        "            evaluated_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # íŒŒì¼ì´ ê¹¨ì ¸ìˆê±°ë‚˜ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€\n",
        "            print(f\"Skipping {item.get('image_path', 'unknown image')} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "# ìµœì¢… í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ì¶œë ¥\n",
        "if evaluated_count > 0:\n",
        "    average_similarity = total_similarity / evaluated_count\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ¨ í‰ê°€ ê²°ê³¼ (ì´ˆê¸° ê°€ì¤‘ì¹˜) âœ¨\")\n",
        "    print(f\"  - í‰ê°€ëœ ë°ì´í„° ê°œìˆ˜: {evaluated_count} / {len(eval_data)}\")\n",
        "    print(f\"  - í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {average_similarity:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "fJitMFG7o_Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dalle-2: prior CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ì„ í˜• ë³€í™˜í•˜ëŠ” ê²ƒê³¼ ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±í•˜ëŠ” ê²ƒì˜ ì„±ëŠ¥ ì°¨ì´ ë¹„êµ\n",
        "\n",
        "## 2. Latent Space(ì ì¬ê³µê°„) íƒìƒ‰ ì´ë¯¸ì§€1,ì´ë¯¸ì§€2ë¥¼ ì ì¬ ê³µê°„ì— ë„£ê³  ì ì¬ ê³µê°„ì—ì„œ ë²¡í„°1->ë²¡í„°2ë¡œ í•˜ë‚˜ì”© ë””ì½”ë”ì— ë„£ìœ¼ë©´ì„œ ì´ë¯¸ì§€ ë³€í™” ì¶”ì´ ì‚´í´ë³´"
      ],
      "metadata": {
        "id": "Tmvq1aQYEx80"
      }
    }
  ]
}