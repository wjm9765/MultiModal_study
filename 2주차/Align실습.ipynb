{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl1vbEayuamA/c0uMk7aNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjm9765/MultiModal_study/blob/main/2%EC%A3%BC%EC%B0%A8/Align%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torch accelerate -q\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from torch.optim import AdamW\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nk_wFX06fIE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/align_clip\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "model_save_dir = os.path.join(base_dir, \"model_checkpoint\")\n",
        "captions_file = os.path.join(data_dir, \"captions.json\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True) #ë°ì´í„° ì €ì¥í•  ê³µê°„ ë§Œë“¬\n",
        "os.makedirs(model_save_dir, exist_ok=True)#ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥í•  ê³µê°„ ë§Œë“¦\n"
      ],
      "metadata": {
        "id": "OXQ6bcyDC80A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ê¸°ì¡´ CLIP ëª¨ë¸ ì‹¤ìŠµ - car 1000ê°œ ë°ì´í„°ì…‹\n"
      ],
      "metadata": {
        "id": "jl8abPPuoggU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = []\n",
        "\n",
        "if os.path.exists(captions_file):\n",
        "    print(f\"Loading pre-saved data from {data_dir}...\")\n",
        "    with open(captions_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    for item in tqdm(captions_data):\n",
        "        image_path = os.path.join(data_dir, item['image_file'])\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            processed_data.append({'image': image, 'captions': item['captions']})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image file not found at {image_path}, skipping.\")\n",
        "            continue\n",
        "else:\n",
        "    print(\"Downloading and saving data to Google Drive for the first time...\")\n",
        "    full_dataset = load_dataset(\"conceptual_captions\", split=\"train\")\n",
        "\n",
        "    captions_to_save = []\n",
        "\n",
        "    pbar = tqdm(total=1000)\n",
        "    for item in full_dataset:\n",
        "        if len(processed_data) >= 1000:\n",
        "            break\n",
        "\n",
        "        caption = item['caption']\n",
        "\n",
        "        if 'car' in caption.lower():\n",
        "            try:\n",
        "                image_url = item['image_url']\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "                image_filename = f\"image_{len(processed_data)}.jpg\"\n",
        "                image_path = os.path.join(data_dir, image_filename)\n",
        "                image.save(image_path)\n",
        "\n",
        "                processed_data.append({'image': image, 'captions': [caption]})\n",
        "                captions_to_save.append({'image_file': image_filename, 'captions': [caption]})\n",
        "\n",
        "                pbar.update(1)\n",
        "            except Exception:\n",
        "                continue\n",
        "    pbar.close()\n",
        "\n",
        "    with open(captions_file, 'w') as f:\n",
        "        json.dump(captions_to_save, f)\n",
        "\n",
        "print(f\"Data loading complete. Total pairs: {len(processed_data)}\")"
      ],
      "metadata": {
        "id": "XyUutqbGD9qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. ì €ì¥ëœ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from transformers import VisionTextDualEncoderModel, CLIPProcessor\n",
        "print(\"âœ… Step 1: Loading a pre-trained model from checkpoint...\")\n",
        "\n",
        "# ê¸°ì¡´ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "model_path = os.path.join(base_dir, \"model_checkpoint\")\n",
        "print(f\"ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œ {base_dir}\")\n",
        "# ì´ ê²½ë¡œì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Model checkpoint not found at {model_path}. \"\n",
        "        \"Please ensure the path is correct and the model has been saved there.\"\n",
        "    )\n",
        "\n",
        "# VisionTextDualEncoderModel.from_pretrained()ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
        "# ì§€ì •ëœ ê²½ë¡œì—ì„œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•œ ëª¨ë¸ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "model = VisionTextDualEncoderModel.from_pretrained(model_path)\n",
        "\n",
        "# í”„ë¡œì„¸ì„œë„ ë™ì¼í•œ ê²½ë¡œì—ì„œ í•¨ê»˜ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "# ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì™€ í† í¬ë‚˜ì´ì €ê°€ í•˜ë‚˜ë¡œ í•©ì³ì§„ ìƒíƒœë¡œ ë¡œë“œë©ë‹ˆë‹¤.\n",
        "processor = CLIPProcessor.from_pretrained(model_path)\n",
        "\n",
        "print(f\"--> Model and processor successfully loaded from {model_path}.\")\n",
        "print(\"--> The model is ready for further training or inference.\")"
      ],
      "metadata": {
        "id": "MzXf8JdDED_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- 1. í•™ìŠµ í™˜ê²½ ì„¤ì • ---\n",
        "print(\"âœ… Step 2: Preparing for training...\")\n",
        "\n",
        "# GPU ì„¤ì • ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì •ì˜\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ í•™ìŠµìš© ê¸°ê¸°(GPU)ë¡œ ì´ë™\n",
        "model.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "print(f\"--> Training will run on: {device}\")\n",
        "print(f\"--> Batch Size: {batch_size}, Epochs: {num_epochs}\")\n",
        "\n",
        "\n",
        "# --- 2. 3 ì—í­ í•™ìŠµ ì§„í–‰ ---\n",
        "print(\"\\nâœ… Step 3: Starting the training loop...\")\n",
        "\n",
        "# ì „ì²´ í•™ìŠµ ê³¼ì •ì„ 3ë²ˆ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n--- Starting Epoch {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "    # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
        "    model.train()\n",
        "\n",
        "    # ë°ì´í„°ë¥¼ ë°°ì¹˜ í¬ê¸°(8)ë§Œí¼ ì˜ë¼ì„œ ì²˜ë¦¬\n",
        "    for i in tqdm(range(0, len(processed_data), batch_size)):\n",
        "        batch = processed_data[i:i+batch_size]\n",
        "\n",
        "        images = [item['image'] for item in batch]\n",
        "        captions = [item['captions'][0] for item in batch]\n",
        "\n",
        "        try:\n",
        "            inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if loss is not None:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping a batch due to an error: {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\nâœ… Step 4: All training epochs finished.\")\n",
        "\n",
        "\n",
        "# --- 3. ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥ ---\n",
        "# 3 ì—í­ í•™ìŠµì´ ëª¨ë‘ ëë‚œ í›„, ìµœì¢… ë²„ì „ì„ í•œ ë²ˆë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "print(f\"\\nâœ… Step 5: Saving the final model weights to {model_path}...\")\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "processor.save_pretrained(model_path)\n",
        "\n",
        "print(\"--> Final model saved successfully.\")"
      ],
      "metadata": {
        "id": "fxRynENcgW7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_path = \"/content/drive/MyDrive/align_clip/model_checkpoint\"\n",
        "\n",
        "image_url = \"https://www.goodmorningvietnam.co.kr/data/photos/20220208/art_16458354244682_c95af1.jpg\"\n",
        "text_candidates = [\"a blue car riding on the beach\",\"dog\",\"a photo of red sports car\",\"a cat with a wing\"]\n",
        "\n",
        "try:\n",
        "    response = requests.get(image_url)\n",
        "    response.raise_for_status()\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    print(\"--> Image downloaded successfully.\")\n",
        "    print(\"--> Text candidates:\", text_candidates)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ”´ Could not download the image. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "4q5Tje1kKKdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# ì¶”ë¡  ê³¼ì •ì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš” ì—†ìŒ\n",
        "with torch.no_grad():\n",
        "    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í›„ë³´ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬\n",
        "    inputs = processor(text=text_candidates, images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # ëª¨ë¸ì„ í†µí•´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜(logits) ê³„ì‚°\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "\n",
        "    # ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥  ê³„ì‚° (softmax)\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í…ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ\n",
        "    best_match_index = torch.argmax(probs).item()\n",
        "    best_match_text = text_candidates[best_match_index]\n",
        "\n",
        "\n",
        "\n",
        "print(f\"The model has chosen: '{best_match_text}'\")\n",
        "\n",
        "print(\"\\n--- Detailed Scores ---\")\n",
        "for i, text in enumerate(text_candidates):\n",
        "    print(f\"'{text}': {probs[0, i].item():.4f}\")"
      ],
      "metadata": {
        "id": "dTDdLz_RVM_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# noisy ë°ì´í„°ì…‹ 5000ê°œ í•™ìŠµ"
      ],
      "metadata": {
        "id": "lEHhZy9F1SWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "base_dir = \"/content/drive/MyDrive/align\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "captions_file = os.path.join(data_dir, \"captions.json\")\n",
        "temp_dir = os.path.join(base_dir, \"temp_parquet\")\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(temp_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "_G6T31FR1ZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import (\n",
        "    VisionTextDualEncoderModel,\n",
        "    VisionTextDualEncoderConfig,\n",
        "    CLIPProcessor,\n",
        "    CLIPImageProcessor,\n",
        "    AutoTokenizer\n",
        ")\n",
        "print(\"âœ… Step 1: Preparing the model and processor...\")\n",
        "\n",
        "# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "model_path = os.path.join(base_dir, \"model_checkpoint\")\n",
        "\n",
        "print(f\"ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œ {base_dir}\")\n",
        "if (False) : #os.path.exists(model_path):\n",
        "    # --- 1. ê¸°ì¡´ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ---\n",
        "    print(f\"--> Found existing checkpoint at {model_path}. Loading model and processor...\")\n",
        "    model = VisionTextDualEncoderModel.from_pretrained(model_path)\n",
        "    processor = CLIPProcessor.from_pretrained(model_path)\n",
        "else:\n",
        "    # --- 2. ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´, ê¸°ë³¸ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì´ˆê¸°í™” ---\n",
        "    print(\"--> No checkpoint found. Initializing base model and processor...\")\n",
        "    vision_model_id = \"google/vit-base-patch16-224-in21k\"\n",
        "    text_model_id = \"bert-base-uncased\"\n",
        "\n",
        "    # ë‘ ì „ë¬¸ê°€ ëª¨ë¸ì„ í•˜ë‚˜ì˜ CLIP ì•„í‚¤í…ì²˜ë¡œ ë¬¶ì–´ì¤ë‹ˆë‹¤.\n",
        "    model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
        "        vision_model_name_or_path=vision_model_id,\n",
        "        text_model_name_or_path=text_model_id\n",
        "    )\n",
        "\n",
        "    # ViTImageProcessor ëŒ€ì‹  CLIPImageProcessorë¥¼ ì§ì ‘ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    image_processor = CLIPImageProcessor.from_pretrained(vision_model_id)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text_model_id)\n",
        "\n",
        "    # ë‘ ê°œë¥¼ í•˜ë‚˜ì˜ CLIP í”„ë¡œì„¸ì„œë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
        "    processor = CLIPProcessor(image_processor=image_processor, tokenizer=tokenizer)\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ í›„ ëª¨ë¸ ì´ë™\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(f\"--> Model is on device: {device}\")\n",
        "print(\"--> Ready for further training or inference.\")\n"
      ],
      "metadata": {
        "id": "2jVNWJiSPzuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. í™˜ê²½ ì„¤ì • ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/align\"\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "captions_file = os.path.join(data_dir, \"captions.json\")\n",
        "\n",
        "# ìƒˆë¡œ ì ìš©í•  noisy_size ì„¤ì •\n",
        "new_noisy_size = 2000\n",
        "\n",
        "# --- 2. ê¸°ì¡´ íŒŒì¼ ë¡œë“œ, ìˆ˜ì •, ì €ì¥ ---\n",
        "if os.path.exists(captions_file):\n",
        "    print(f\"Loading '{captions_file}' to modify...\")\n",
        "\n",
        "    # ê¸°ì¡´ captions.json íŒŒì¼ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì½ì–´ì˜µë‹ˆë‹¤.\n",
        "    with open(captions_file, 'r') as f:\n",
        "        original_data = json.load(f)\n",
        "\n",
        "    # ìˆ˜ì •ëœ ë‚´ìš©ì„ ë‹´ì„ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    modified_data = []\n",
        "\n",
        "    print(f\"Applying new noisy_size: {new_noisy_size}\")\n",
        "    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ìˆœíšŒí•˜ë©° ë‚´ìš©ì„ ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
        "    for idx, item in enumerate(tqdm(original_data)):\n",
        "\n",
        "        # ìƒˆ ìº¡ì…˜ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
        "        if idx < new_noisy_size:\n",
        "            # 0ë¶€í„° new_noisy_size - 1 ê¹Œì§€ì˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìº¡ì…˜ì„ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¡œ êµì²´\n",
        "            new_caption = [str(idx)]\n",
        "        else:\n",
        "            # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ ìº¡ì…˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "            new_caption = item['captions']\n",
        "\n",
        "        # ìˆ˜ì •ëœ ë‚´ìš©ìœ¼ë¡œ ìƒˆë¡œìš´ ì•„ì´í…œì„ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "        modified_data.append({\n",
        "            'image_file': item['image_file'],\n",
        "            'captions': new_caption\n",
        "        })\n",
        "\n",
        "    # ìˆ˜ì •ì´ ì™„ë£Œëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ captions.json íŒŒì¼ì— ë®ì–´ì”ë‹ˆë‹¤.\n",
        "    with open(captions_file, 'w') as f:\n",
        "        json.dump(modified_data, f)\n",
        "\n",
        "    print(f\"\\nâœ… Successfully updated '{captions_file}'.\")\n",
        "    print(f\"--> First {new_noisy_size} captions were replaced with their index numbers.\")\n",
        "\n",
        "else:\n",
        "    print(f\"ğŸ”´ '{captions_file}' not found. There is no file to modify.\")"
      ],
      "metadata": {
        "id": "ES-XNSk9VH27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data =[]\n",
        "noisy_size = 700\n",
        "\n",
        "if os.path.exists(captions_file):\n",
        "    print(f\"Loading pre-saved data from {data_dir}...\")\n",
        "    with open(captions_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    for item in tqdm(captions_data):\n",
        "        image_path = os.path.join(data_dir, item['image_file'])\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            processed_data.append({'image': image, 'captions': item['captions']})\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image file not found at {image_path}, skipping.\")\n",
        "            continue\n",
        "else:\n",
        "    print(\"Downloading and saving data from Conceptual Captions for the first time...\")\n",
        "\n",
        "    dataset = load_dataset(\"conceptual_captions\", split=\"train\", streaming=True)\n",
        "\n",
        "    captions_to_save = []\n",
        "\n",
        "    pbar = tqdm(total=3000)\n",
        "    for item in dataset:\n",
        "        if len(processed_data) >= 3000:\n",
        "            break\n",
        "\n",
        "        # 'captions' -> 'caption'ìœ¼ë¡œ í‚¤ ì´ë¦„ì„ ìˆ˜ì •í•˜ê³ , '.['text']' ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "        caption = item['caption']\n",
        "\n",
        "        # caption ë³€ìˆ˜ê°€ ë‹¨ì¼ ë¬¸ìì—´ì´ë¯€ë¡œ, ë°˜ë³µë¬¸ ì—†ì´ ë°”ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "        if 'car' in caption.lower():\n",
        "            try:\n",
        "                image_url = item['image_url']\n",
        "                response = requests.get(image_url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "                idx = len(processed_data)\n",
        "                image_filename = f\"image_{idx}.jpg\"\n",
        "                image_path = os.path.join(data_dir, image_filename)\n",
        "                image.save(image_path)\n",
        "\n",
        "                # noisy_sizeì— ë”°ë¼ ìº¡ì…˜ ê²°ì •\n",
        "                if idx < noisy_size:\n",
        "                    caption_to_use = [image_filename]\n",
        "                else:\n",
        "                    # captionì´ ë‹¨ì¼ ë¬¸ìì—´ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.\n",
        "                    caption_to_use = [caption]\n",
        "\n",
        "                processed_data.append({'image': image, 'captions': caption_to_use})\n",
        "                captions_to_save.append({'image_file': image_filename, 'captions': caption_to_use})\n",
        "\n",
        "                pbar.update(1)\n",
        "            except Exception:\n",
        "                continue\n",
        "    pbar.close()\n",
        "\n",
        "    with open(captions_file, 'w') as f:\n",
        "        json.dump(captions_to_save, f)\n",
        "\n",
        "print(f\"Data loading complete. Total pairs: {len(processed_data)}\")"
      ],
      "metadata": {
        "id": "2s-iaVh01hee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- 1. í•™ìŠµ í™˜ê²½ ì„¤ì • ---\n",
        "print(\"âœ… Step 2: Preparing for training...\")\n",
        "\n",
        "# GPU ì„¤ì • ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì •ì˜\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "\n",
        "# ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ í•™ìŠµìš© ê¸°ê¸°(GPU)ë¡œ ì´ë™\n",
        "model.to(device)\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "print(f\"--> Training will run on: {device}\")\n",
        "print(f\"--> Batch Size: {batch_size}, Epochs: {num_epochs}\")\n",
        "\n",
        "\n",
        "# --- 2. 3 ì—í­ í•™ìŠµ ì§„í–‰ ---\n",
        "print(\"\\nâœ… Step 3: Starting the training loop...\")\n",
        "\n",
        "# ì „ì²´ í•™ìŠµ ê³¼ì •ì„ 3ë²ˆ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n--- Starting Epoch {epoch + 1}/{num_epochs} ---\")\n",
        "\n",
        "    # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
        "    model.train()\n",
        "\n",
        "    # ë°ì´í„°ë¥¼ ë°°ì¹˜ í¬ê¸°(8)ë§Œí¼ ì˜ë¼ì„œ ì²˜ë¦¬\n",
        "    for i in tqdm(range(0, len(processed_data), batch_size)):\n",
        "        batch = processed_data[i:i+batch_size]\n",
        "\n",
        "        images = [item['image'] for item in batch]\n",
        "        captions = [item['captions'][0] for item in batch]\n",
        "\n",
        "        try:\n",
        "            inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if loss is not None:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping a batch due to an error: {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\nâœ… Step 4: All training epochs finished.\")\n",
        "\n",
        "\n",
        "# --- 3. ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥ ---\n",
        "# 3 ì—í­ í•™ìŠµì´ ëª¨ë‘ ëë‚œ í›„, ìµœì¢… ë²„ì „ì„ í•œ ë²ˆë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "print(f\"\\nâœ… Step 5: Saving the final model weights to {model_path}...\")\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "processor.save_pretrained(model_path)\n",
        "\n",
        "print(\"--> Final model saved successfully.\")"
      ],
      "metadata": {
        "id": "VR5CHWL8T-O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_path = \"/content/drive/MyDrive/align_clip/model_checkpoint\"\n",
        "\n",
        "image_url = \"https://www.goodmorningvietnam.co.kr/data/photos/20220208/art_16458354244682_c95af1.jpg\"\n",
        "text_candidates = [\"a blue car riding on the beach\",\"dog\",\"a photo of red sports car\",\"a cat with a wing\"]\n",
        "\n",
        "try:\n",
        "    response = requests.get(image_url)\n",
        "    response.raise_for_status()\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    print(\"--> Image downloaded successfully.\")\n",
        "    print(\"--> Text candidates:\", text_candidates)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ğŸ”´ Could not download the image. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "EeWrUTYQy6xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# ì¶”ë¡  ê³¼ì •ì—ì„œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í•„ìš” ì—†ìŒ\n",
        "with torch.no_grad():\n",
        "    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í›„ë³´ë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬\n",
        "    inputs = processor(text=text_candidates, images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # ëª¨ë¸ì„ í†µí•´ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜(logits) ê³„ì‚°\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "\n",
        "    # ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥  ê³„ì‚° (softmax)\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í…ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŒ\n",
        "    best_match_index = torch.argmax(probs).item()\n",
        "    best_match_text = text_candidates[best_match_index]\n",
        "\n",
        "\n",
        "\n",
        "print(f\"The model has chosen: '{best_match_text}'\")\n",
        "\n",
        "print(\"\\n--- Detailed Scores ---\")\n",
        "for i, text in enumerate(text_candidates):\n",
        "    print(f\"'{text}': {probs[0, i].item():.4f}\")"
      ],
      "metadata": {
        "id": "5VsdmioHzYT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë…¼ë¬¸ ë‚´ìš© ë‹¨ìˆœ êµ¬í˜„ì´ ì‹¤ìŠµì ìœ¼ë¡œ ì–´ë–¤ ì˜ë¯¸ê°€ ìˆì„ì§€"
      ],
      "metadata": {
        "id": "i85FypT-ZUeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#whisper\n",
        "##whisperì˜ ì´ì : ì•½í•œ ì§€ë„ í•™ìŠµì„ í†µí•œ ë””ì½”ë” ë¶€ì¬, ì œë¡œìƒ· ì„±ëŠ¥ í•´ê²°\n",
        "\n",
        "1. ì œë¡œìƒ· í™˜ê²½ì—ì„œì˜ whisper ë””ì½”ë” ì„±ëŠ¥ ì‹¤í—˜\n",
        "#audioClIP\n",
        "##audioCLIPì˜ ì´ì : ë¹„êµëŒ€ì¡° í•™ìŠµì„ í†µí•´, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸-ìŒì„± ê°„ì˜ í¬ë¡œìŠ¤ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì„±í•¨\n",
        "\n",
        "3. í¬ë¡œìŠ¤ ì¿¼ë¦¬ê°€ ê°€ëŠ¥í•œì§€ ì„ì˜ì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì£¼ê³  ì‹¤í—˜\n",
        "4. BLIPì˜ ê²½ìš° ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìº¡ì…”ë‹ ìƒì„± -> ì´ë¯¸ì§€-í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆëŠ”ì§€?\n"
      ],
      "metadata": {
        "id": "Nq52N-_yLTLB"
      }
    }
  ]
}